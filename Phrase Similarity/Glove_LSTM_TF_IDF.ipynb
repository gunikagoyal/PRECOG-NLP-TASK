{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GCG7embt5i8Q8GHlQlPnrxly35RDP84S",
      "authorship_tag": "ABX9TyNbMruUdUDlIL1gRds8HgY4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4mMOqzrlDHG",
        "outputId": "a227eaa1-a6bc-48ac-e9d3-7d43f17f10ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yLgZtNXKJb",
        "outputId": "9c07c0d5-7a10-4a2a-f8e7-bbb8798ff141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7b1ieXoZBQ7",
        "outputId": "56f17080-3ca3-4f93-978a-4c92595f07a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GloVe word embeddings is loaded from Google Drive\n",
        "def load_glove_embeddings_from_drive(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "#Main function\n",
        "def main():\n",
        "    glove_embeddings_path = '/content/drive/MyDrive/glove.6B.200d.txt'\n",
        "    glove_embeddings = load_glove_embeddings_from_drive(glove_embeddings_path)\n"
      ],
      "metadata": {
        "id": "Rj5fNfu0ar1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GloVe Embeddings Using Siamese Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "zLHGbpkYnIgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Dropout, Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from datasets import load_dataset\n",
        "\n",
        "def load_glove_embeddings_from_drive(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "#Load dataset from Hugging Face\n",
        "def load_dataset_from_huggingface():\n",
        "    dataset = load_dataset(\"PiC/phrase_similarity\")\n",
        "    return pd.DataFrame(dataset['test'])\n",
        "\n",
        "#Function to convert a phrase to its vector representation\n",
        "def phrase_to_vector(phrase, word_embeddings):\n",
        "    words = phrase.split()\n",
        "    vector = np.zeros_like(word_embeddings[next(iter(word_embeddings))])\n",
        "    for word in words:\n",
        "        if word in word_embeddings:\n",
        "            vector += word_embeddings[word]\n",
        "    return vector / len(words)\n",
        "\n",
        "#Prepare data\n",
        "def prepare_data(dataset, word_embeddings):\n",
        "    phrase1 = dataset['phrase1']\n",
        "    phrase2 = dataset['phrase2']\n",
        "    similarity_score = dataset['label']\n",
        "\n",
        "    X1 = np.array([phrase_to_vector(phrase, word_embeddings) for phrase in phrase1])\n",
        "    X2 = np.array([phrase_to_vector(phrase, word_embeddings) for phrase in phrase2])\n",
        "    y = similarity_score.values\n",
        "    return X1, X2, y\n",
        "\n",
        "#Defining neural network model\n",
        "def create_model(input_dim):\n",
        "    input1 = Input(shape=(input_dim,))\n",
        "    input2 = Input(shape=(input_dim,))\n",
        "\n",
        "    #Defining individual branches for each input\n",
        "    branch1 = Dense(128, activation='relu')(input1)\n",
        "    branch2 = Dense(128, activation='relu')(input2)\n",
        "\n",
        "    #Concatenate the outputs of the two branches\n",
        "    merged = Concatenate()([branch1, branch2])\n",
        "\n",
        "    #Additional layers\n",
        "    merged = Dropout(0.2)(merged)\n",
        "    merged = Dense(64, activation='relu')(merged)\n",
        "    output = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "    #Defining the model\n",
        "    model = Model(inputs=[input1, input2], outputs=output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#Main function\n",
        "def main():\n",
        "    #Loading GloVe embeddings from Google Drive\n",
        "    glove_embeddings_path = '/content/drive/MyDrive/glove.6B.200d.txt'\n",
        "    glove_embeddings = load_glove_embeddings_from_drive(glove_embeddings_path)\n",
        "\n",
        "    #Loading dataset from Hugging Face\n",
        "    dataset = load_dataset_from_huggingface()\n",
        "\n",
        "    #Preparing data\n",
        "    X1, X2, y = prepare_data(dataset, glove_embeddings)\n",
        "\n",
        "    #Train-test split\n",
        "    X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Define model\n",
        "    input_dim = X1_train.shape[1]\n",
        "    model = create_model(input_dim)\n",
        "\n",
        "    #Training model\n",
        "    model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    #Evaluating model\n",
        "    y_pred = model.predict([X1_test, X2_test])\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaMrFf2nbC9Z",
        "outputId": "81afe42a-22cc-4b9e-d05f-bf464934981c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "45/45 [==============================] - 2s 9ms/step - loss: 0.7096 - accuracy: 0.5097 - val_loss: 0.6878 - val_accuracy: 0.5688\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5958 - val_loss: 0.7054 - val_accuracy: 0.4812\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.6361 - val_loss: 0.6933 - val_accuracy: 0.5437\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6944 - val_loss: 0.6946 - val_accuracy: 0.5437\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 0.5568 - accuracy: 0.7194 - val_loss: 0.7733 - val_accuracy: 0.4812\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.7688 - val_loss: 0.7903 - val_accuracy: 0.5625\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8000 - val_loss: 0.7875 - val_accuracy: 0.5562\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.3997 - accuracy: 0.8368 - val_loss: 0.8429 - val_accuracy: 0.5250\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.8493 - val_loss: 0.8802 - val_accuracy: 0.5688\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 0s 4ms/step - loss: 0.3479 - accuracy: 0.8521 - val_loss: 1.0097 - val_accuracy: 0.5312\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.4775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "q0HKslCFnRkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from datasets import load_dataset\n",
        "from keras.layers import Input, LSTM, Dense, Lambda\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "#Load dataset from Hugging Face\n",
        "def load_dataset_from_huggingface():\n",
        "    dataset = load_dataset(\"PiC/phrase_similarity\")\n",
        "    return pd.DataFrame(dataset['test'])\n",
        "\n",
        "def siamese_network_model(input_dim, max_len):\n",
        "    input_1 = Input(shape=(max_len,))\n",
        "    input_2 = Input(shape=(max_len,))\n",
        "\n",
        "    #Shared LSTM layer\n",
        "    shared_lstm = LSTM(128)\n",
        "\n",
        "    #Output embeddings\n",
        "    output_1 = shared_lstm(input_1[:, :, None])\n",
        "    output_2 = shared_lstm(input_2[:, :, None])\n",
        "\n",
        "    #Euclidean distance layer\n",
        "    distance = Lambda(lambda x: K.abs(x[0] - x[1]))([output_1, output_2])\n",
        "\n",
        "    #Final prediction layer\n",
        "    output = Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#Tokenize and pad sequences\n",
        "def tokenize_and_pad(texts, max_len):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "    return padded_sequences, tokenizer.word_index\n",
        "\n",
        "#Main function\n",
        "def main():\n",
        "    # Load dataset\n",
        "    dataset = load_dataset_from_huggingface()\n",
        "\n",
        "    #Prepare data\n",
        "    X = dataset[['phrase1', 'phrase2']]\n",
        "    y = dataset['label']\n",
        "\n",
        "    #Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Tokenize and pad sequences for Siamese Network\n",
        "    max_len = max(max(len(phrase.split()) for phrase in X_train['phrase1']),\n",
        "                  max(len(phrase.split()) for phrase in X_train['phrase2']))\n",
        "    X_train_seq_1, word_index = tokenize_and_pad(X_train['phrase1'], max_len)\n",
        "    X_train_seq_2, _ = tokenize_and_pad(X_train['phrase2'], max_len)\n",
        "    X_test_seq_1, _ = tokenize_and_pad(X_test['phrase1'], max_len)\n",
        "    X_test_seq_2, _ = tokenize_and_pad(X_test['phrase2'], max_len)\n",
        "\n",
        "    #Concatenate sequences\n",
        "    X_train_seq = np.concatenate((X_train_seq_1, X_train_seq_2), axis=1)\n",
        "    X_test_seq = np.concatenate((X_test_seq_1, X_test_seq_2), axis=1)\n",
        "\n",
        "    #Siamese Network (LSTM) Model\n",
        "    model = siamese_network_model(X_train_seq.shape[1], max_len)\n",
        "\n",
        "    #Train model\n",
        "    model.fit([X_train_seq[:, :max_len], X_train_seq[:, max_len:]], y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
        "\n",
        "    #Evaluate model\n",
        "    y_pred = (model.predict([X_test_seq[:, :max_len], X_test_seq[:, max_len:]]) > 0.5).astype(int)\n",
        "    accuracy_siamese = accuracy_score(y_test, y_pred)\n",
        "    report_siamese = classification_report(y_test, y_pred)\n",
        "\n",
        "    print(\"Siamese Network (LSTM) Model:\")\n",
        "    print(\"Accuracy:\", accuracy_siamese)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report_siamese)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL6eJ-45uU3r",
        "outputId": "dd474a55-6e11-4fc8-bd7e-e13d02f74318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "45/45 [==============================] - 8s 77ms/step - loss: 0.6950 - accuracy: 0.5056 - val_loss: 0.6905 - val_accuracy: 0.5562\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 1s 29ms/step - loss: 0.6933 - accuracy: 0.5035 - val_loss: 0.7085 - val_accuracy: 0.4250\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.6911 - accuracy: 0.5312 - val_loss: 0.6966 - val_accuracy: 0.4625\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 2s 35ms/step - loss: 0.6898 - accuracy: 0.5243 - val_loss: 0.6991 - val_accuracy: 0.4437\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 2s 36ms/step - loss: 0.6893 - accuracy: 0.5410 - val_loss: 0.6921 - val_accuracy: 0.4625\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 1s 23ms/step - loss: 0.6916 - accuracy: 0.5194 - val_loss: 0.6826 - val_accuracy: 0.5375\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.6895 - accuracy: 0.5160 - val_loss: 0.6959 - val_accuracy: 0.4437\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.6862 - accuracy: 0.5576 - val_loss: 0.7158 - val_accuracy: 0.4250\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 1s 18ms/step - loss: 0.6874 - accuracy: 0.5417 - val_loss: 0.6975 - val_accuracy: 0.4500\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 1s 19ms/step - loss: 0.6861 - accuracy: 0.5368 - val_loss: 0.7074 - val_accuracy: 0.4250\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Siamese Network (LSTM) Model:\n",
            "Accuracy: 0.4925\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.23      0.31       204\n",
            "           1       0.49      0.77      0.60       196\n",
            "\n",
            "    accuracy                           0.49       400\n",
            "   macro avg       0.50      0.50      0.45       400\n",
            "weighted avg       0.50      0.49      0.45       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Representation with Random Forest Classifier**"
      ],
      "metadata": {
        "id": "y3wdg4GTnb5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Lambda, Conv1D, GlobalMaxPooling1D, Dropout, Concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from keras import backend as K\n",
        "\n",
        "#Load dataset from Hugging Face\n",
        "def load_dataset_from_huggingface():\n",
        "    dataset = load_dataset(\"PiC/phrase_similarity\")\n",
        "    return pd.DataFrame(dataset['test'])\n",
        "\n",
        "#TF-IDF Representation with Random Forest Classifier\n",
        "def tfidf_rf_model(X_train, X_test, y_train, y_test):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    #Train Random Forest classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    #Predict and evaluate\n",
        "    y_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
        "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "    return accuracy_rf, report_rf\n",
        "\n",
        "\n",
        "#Tokenize and pad sequences\n",
        "def tokenize_and_pad(texts, max_len):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "    return padded_sequences, tokenizer.word_index\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    dataset = load_dataset_from_huggingface()\n",
        "\n",
        "    #Prepare data\n",
        "    X = dataset[['phrase1', 'phrase2']]\n",
        "    y = dataset['label']\n",
        "\n",
        "    #Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #TF-IDF Representation with Random Forest Classifier\n",
        "    accuracy_rf, report_rf = tfidf_rf_model(X_train['phrase1'] + X_train['phrase2'],\n",
        "                                            X_test['phrase1'] + X_test['phrase2'],\n",
        "                                            y_train,\n",
        "                                            y_test)\n",
        "\n",
        "    print(\"TF-IDF Representation with Random Forest Classifier:\")\n",
        "    print(\"Accuracy:\", accuracy_rf)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report_rf)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "9ljRp37Rn52n",
        "outputId": "5fca34b8-a78d-48c5-d6aa-4bbba8449cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation with Random Forest Classifier:\n",
            "Accuracy: 0.45\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.44      0.45       204\n",
            "           1       0.44      0.46      0.45       196\n",
            "\n",
            "    accuracy                           0.45       400\n",
            "   macro avg       0.45      0.45      0.45       400\n",
            "weighted avg       0.45      0.45      0.45       400\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f9af5a11d570>\u001b[0m in \u001b[0;36m<cell line: 152>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-f9af5a11d570>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# Siamese Network (LSTM) Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f9af5a11d570>\u001b[0m in \u001b[0;36msiamese_network_model\u001b[0;34m(input_dim)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Output embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 8)"
          ]
        }
      ]
    }
  ]
}
